{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shuQbOZfPrBj"
      },
      "source": [
        "## Short Risks and Opportunities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy9NeDLfPvJU"
      },
      "source": [
        "This model is fairly simple, here we will look at a range of important correlations to determine whether a stock is under-or over-shorted. And under-shorted stock has the potential to decrease in price in the near future (1-2 years), and an over-shorted stock has the potential to increase in price in the near future (1-2 years), due to the risk of a short-squeeze. This model is more proprietory in nature. Daily data could have its place, but it doesn't make too much sense yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow-K8p1Pk_on"
      },
      "source": [
        "SHORTINT reflects positions held on the 15th business day of each month. SHORTINTME reflects position held on the last business day of the month. Hence, these variables reflect the evolution of short interest during the respective time periods. Population coverage includes the New York Stock Exchange, American Stock Exchange, and NASDAQ. This item is provided to Compustat by FT Interactive. It seems like the SEC actually carries short-interest data. You can always cik up and then ticker up to see if you can add even more connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chJdcbwAUAN9"
      },
      "outputs": [],
      "source": [
        "### Note this has some problems, that short interest file might take very long to come out\n",
        "### For example it has been a week, another problem is that if there is a problem in the\n",
        "### Accounting and ratios feauture store then of course it would affect this feature store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlS4qJVDlvHj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install wrds\n",
        "\n",
        "# import wrds\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "## Ratios: https://wrds-www.wharton.upenn.edu/pages/support/research-wrds/sample-programs/wrds-sample-programs/wrds-financial-ratios-suite/\n",
        "## concept: https://wrds-www.wharton.upenn.edu/pages/browse-data-concept/\n",
        "## Research Applications Code: https://wrds-www.wharton.upenn.edu/pages/support/applications/\n",
        "\n",
        "# db = wrds.Connection()\n",
        "\n",
        "# df_short_raw = db.raw_sql(\"select * from comp.sec_shortint where datadate between '1960-01-01' and '2024-06-07'\")\n",
        "# df_short_raw.to_parquet(\"gs://sovai-short/short_interest/sec_short_wrds.parquet\")\n",
        "\n",
        "# df_security = db.raw_sql(\"select * from comp.security\")\n",
        "# df_security.to_parquet(\"gs://sovai-short/short_interest/sec_master.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "HqlRD1HIUI_W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ro7kIAVUAN-",
        "outputId": "2470f131-01cd-4d94-da3d-54245ac4d0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.11.14)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2025.3.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.18.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2025.1.31)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.11.14)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2025.3.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.18.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2025.1.31)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (1.21.0)\n",
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.11/dist-packages (2024.11.0)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.0.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.9.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting s5cmd\n",
            "  Downloading s5cmd-0.2.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Downloading s5cmd-0.2.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: s5cmd\n",
            "Successfully installed s5cmd-0.2.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.11.14)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2025.3.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.32.3)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.18.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.9.1->oauth2client) (3.2.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2025.1.31)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.37.25-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Collecting botocore<1.38.0,>=1.37.25 (from boto3)\n",
            "  Downloading botocore-1.37.25-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.25->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.25->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.25->boto3) (1.17.0)\n",
            "Downloading boto3-1.37.25-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.25-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m135.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.37.25 botocore-1.37.25 jmespath-1.0.1 s3transfer-0.11.4\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install polars scikit-learn pyarrow regex pandas gcsfs numpy fastparquet lightgbm>=4.0.0 bigframes\n",
        "\n",
        "!pip install pyarrow>=11.0.0\n",
        "!pip install lightgbm>=4.0.0\n",
        "!pip install gcsfs\n",
        "!pip install gcsfs\n",
        "!pip install pyarrow\n",
        "!pip install polars\n",
        "!pip install fastparquet\n",
        "!pip install pandas\n",
        "!pip install s5cmd\n",
        "!pip install tqdm gcsfs gspread oauth2client\n",
        "!pip install boto3 duckdb\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nJujZwWMjW-o",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Owuc4fWKUAN_",
        "outputId": "7bd8bd00-3e5f-4afe-e4a1-d53c11708978"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Query job 3f952cb2-b30d-425d-977f-bae5f3b4f123 is DONE. 169.8 MB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=alert-howl-193320&j=bq:australia-southeast2:3f952cb2-b30d-425d-977f-bae5f3b4f123&page=queryresults\">Open Job</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Query job 647bb0f3-cd56-4c63-a47f-8f428878fdb6 is DONE. 169.8 MB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=alert-howl-193320&j=bq:australia-southeast2:647bb0f3-cd56-4c63-a47f-8f428878fdb6&page=queryresults\">Open Job</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "48390"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import bigframes.pandas as bpd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "#### GRAB SHORT DATA\n",
        "\n",
        "df_short_raw = pd.read_parquet(\"gs://sovai-short/short_interest/sec_short_wrds.parquet\")\n",
        "df_security = pd.read_parquet(\"gs://sovai-short/short_interest/sec_master.parquet\")\n",
        "df_short_raw = df_short_raw[[\"gvkey\",\"iid\"]].drop_duplicates()\n",
        "df_short_raw = df_short_raw.merge(df_security, on=[\"gvkey\",\"iid\"], how=\"left\")\n",
        "df_short_raw = df_short_raw.drop_duplicates([\"tic\"])\n",
        "df_short = pd.read_parquet(\"gs://sovai-short/short_interest/sec_short_wrds.parquet\")\n",
        "df_short_raw = df_short_raw.sort_values([\"gvkey\",\"iid\"])\n",
        "# Find duplicate rows based on the 'gvkey' column\n",
        "# duplicate_rows = df_short_raw[df_short_raw.duplicated(subset=['gvkey'], keep=False)]\n",
        "\n",
        "df_short = df_short.merge(df_short_raw[[\"gvkey\",\"iid\",\"tic\",\"cusip\"]], on=[\"gvkey\",\"iid\"], how=\"left\")\n",
        "df_short = df_short.drop(columns=[\"gvkey\",\"iid\"])\n",
        "df_short = df_short[['tic', 'cusip'] + [col for col in df_short.columns if col not in ['tic', 'cusip']]]\n",
        "df_short = df_short.drop_duplicates([\"tic\",\"datadate\"], keep=\"first\")\n",
        "df_short.head()\n",
        "\n",
        "\n",
        "# Note this, is just sell not short sell\n",
        "project_id = 'alert-howl-193320'\n",
        "\n",
        "# Set BigQuery DataFrames options\n",
        "bpd.options.bigquery.project = project_id\n",
        "bpd.options.bigquery.location = \"australia-southeast2\"\n",
        "\n",
        "# Get yesterday's date\n",
        "# yesterday = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "\n",
        "# WHERE date = DATE('{yesterday}')\n",
        "\n",
        "# Query for short_data_by_exchange table\n",
        "short_data_by_exchange_query = f'''\n",
        "    SELECT *\n",
        "    FROM `alert-howl-193320.short_data.finra_short_interest`\n",
        "'''\n",
        "\n",
        "# Load data from the short_data table using BigFrames DataFrames\n",
        "short_interest_df = bpd.read_gbq(short_data_by_exchange_query)\n",
        "short_interest_df = short_interest_df.to_pandas()\n",
        "short_interest_df = short_interest_df.sort_values([\"symbol\",\"settlement_date\"])\n",
        "short_interest_df = short_interest_df[[\"symbol\",\"settlement_date\",\"short_interest\"]]\n",
        "short_interest_df.columns = [\"ticker\",\"settlement_date\",\"short_interest\"]\n",
        "\n",
        "def short_move(short_interest_df):\n",
        "  # Add two weeks to the settlement_date column\n",
        "  short_interest_df[\"date\"] = pd.to_datetime(short_interest_df[\"settlement_date\"])\n",
        "  short_interest_df[\"date\"] += pd.DateOffset(weeks=2)\n",
        "\n",
        "  # Vectorized approach to resample dates to the next Friday\n",
        "  short_interest_df[\"date\"] = short_interest_df[\"date\"] + pd.offsets.Week(weekday=4)\n",
        "  short_interest_df[\"date\"] = short_interest_df[\"date\"] - pd.to_timedelta(short_interest_df[\"date\"].dt.dayofweek, unit='d')\n",
        "\n",
        "  # Set the index to [\"ticker\", \"date\"] for resampling\n",
        "  short_interest_df = short_interest_df.set_index([\"ticker\", \"date\"])\n",
        "\n",
        "  # Resample the DataFrame on a weekly basis ending on Friday using the new \"date\" column\n",
        "  # short_interest_df = short_interest_df.groupby([\"ticker\", pd.Grouper(level=\"date\", freq=\"W-FRI\")]).last()\n",
        "\n",
        "  short_interest_df = short_interest_df.groupby(level=\"ticker\").resample(\"W-FRI\", level=\"date\").last()\n",
        "\n",
        "  # Reset the index to make \"ticker\" and \"date\" regular columns\n",
        "  short_interest_df = short_interest_df.reset_index()\n",
        "\n",
        "  short_interest_df = short_interest_df.set_index([\"ticker\", \"date\"])\n",
        "  short_interest_df = short_interest_df.groupby(\"ticker\").ffill()\n",
        "  short_interest_df = short_interest_df.reset_index()\n",
        "  return short_interest_df\n",
        "\n",
        "short_interest_df = short_move(short_interest_df)\n",
        "\n",
        "df_short.head()\n",
        "df_short = df_short[[\"tic\",\"shortint\",\"datadate\"]]\n",
        "\n",
        "df_short.columns = [\"ticker\",\"short_interest\",\"settlement_date\"]\n",
        "\n",
        "df_short = df_short[[\"ticker\",\"settlement_date\",\"short_interest\"]]\n",
        "\n",
        "df_short = short_move(df_short)\n",
        "df_short.shape\n",
        "short_interest_df.shape\n",
        "df_short = pd.concat([df_short,short_interest_df],axis=0).sort_values([\"ticker\",\"date\"]).drop_duplicates([\"ticker\",\"date\"], keep=\"first\")\n",
        "# import pandas as pd\n",
        "\n",
        "# # Assuming you have already performed the resampling and forward fill operations on short_interest_df\n",
        "\n",
        "# # Get the unique tickers from short_interest_df\n",
        "# tickers_to_keep = short_interest_df[\"ticker\"].unique()\n",
        "\n",
        "# # Select only the tickers in df_short that are also present in short_interest_df\n",
        "# df_short = df_short[df_short[\"ticker\"].isin(tickers_to_keep)]\n",
        "\n",
        "df_short = df_short.reset_index(drop=True)\n",
        "len(df_short[\"ticker\"].unique())\n",
        "#### GRAB PRICING DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "u_K5b--GUAOA",
        "outputId": "66c8dd5b-fe3d-4f11-ac12-9f2f43f86d2f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ticker</th>\n",
              "      <th>date</th>\n",
              "      <th>settlement_date</th>\n",
              "      <th>short_interest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>268322</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2003-08-01</td>\n",
              "      <td>2003-07-15</td>\n",
              "      <td>14349976.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268323</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2003-08-08</td>\n",
              "      <td>2003-07-15</td>\n",
              "      <td>14349976.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268324</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2003-08-15</td>\n",
              "      <td>2003-07-15</td>\n",
              "      <td>14349976.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268325</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2003-08-22</td>\n",
              "      <td>2003-07-15</td>\n",
              "      <td>14349976.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268326</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2003-08-29</td>\n",
              "      <td>2003-07-15</td>\n",
              "      <td>14349976.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269423</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2024-09-06</td>\n",
              "      <td>2024-08-15</td>\n",
              "      <td>121598771.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269424</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2024-09-13</td>\n",
              "      <td>2024-08-15</td>\n",
              "      <td>121598771.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269425</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2024-09-20</td>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>135042504.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269426</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2024-09-27</td>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>135042504.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269427</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2024-10-04</td>\n",
              "      <td>2024-09-13</td>\n",
              "      <td>130539888.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1106 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ticker       date settlement_date  short_interest\n",
              "268322   AAPL 2003-08-01      2003-07-15      14349976.0\n",
              "268323   AAPL 2003-08-08      2003-07-15      14349976.0\n",
              "268324   AAPL 2003-08-15      2003-07-15      14349976.0\n",
              "268325   AAPL 2003-08-22      2003-07-15      14349976.0\n",
              "268326   AAPL 2003-08-29      2003-07-15      14349976.0\n",
              "...       ...        ...             ...             ...\n",
              "269423   AAPL 2024-09-06      2024-08-15     121598771.0\n",
              "269424   AAPL 2024-09-13      2024-08-15     121598771.0\n",
              "269425   AAPL 2024-09-20      2024-08-30     135042504.0\n",
              "269426   AAPL 2024-09-27      2024-08-30     135042504.0\n",
              "269427   AAPL 2024-10-04      2024-09-13     130539888.0\n",
              "\n",
              "[1106 rows x 4 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_short.query(\"ticker == 'AAPL'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "Wu36UfFU8o7l",
        "outputId": "e02d775f-2d0f-4d58-cc91-50555d4005e1",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(44059497,)\n",
            "(44058202,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "import gcsfs\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "\n",
        "df_pricing = pd.read_parquet(\"gs://sovai-accounting/dataframes/prices.parq\")\n",
        "\n",
        "float_columns = ['open', 'high', 'low', 'close', 'volume', 'closeadj', 'closeunadj']\n",
        "for col in float_columns:\n",
        "    df_pricing[col] = df_pricing[col].astype('float32')\n",
        "\n",
        "df_pricing['date'] = pd.to_datetime(df_pricing['date']).dt.normalize()\n",
        "df_pricing.shape\n",
        "\n",
        "df_pricing = df_pricing.drop(columns=[\"lastupdated\"])\n",
        "df_pricing = df_pricing.reset_index(drop=True)\n",
        "\n",
        "df_pricing = df_pricing[~df_pricing[\"ticker\"].isnull()]\n",
        "\n",
        "print(df_pricing[\"ticker\"].shape)\n",
        "ticker_counts = df_pricing[\"ticker\"].value_counts()\n",
        "\n",
        "# Filter tickers with at least 30 occurrences\n",
        "tickers_to_keep = ticker_counts[ticker_counts >= 30].index\n",
        "\n",
        "# Filter the DataFrame to keep only the desired tickers\n",
        "df_pricing = df_pricing[df_pricing[\"ticker\"].isin(tickers_to_keep)]\n",
        "print(df_pricing[\"ticker\"].shape)\n",
        "\n",
        "df_pricing = df_pricing.reset_index(drop=True)\n",
        "\n",
        "# Perfect, so this is adjusted data.\n",
        "# Open, High, Low, Close and Volume\n",
        "# These fields are adjusted for stock splits and stock dividends. They are not adjusted for cash dividends or spinoffs.\n",
        "df_pricing = df_pricing.drop(columns=[\"closeadj\",\"closeunadj\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pTPLdhYEUAOA",
        "outputId": "adfa33e0-1959-46e5-9b33-101d410d20e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ticker</th>\n",
              "      <th>date</th>\n",
              "      <th>settlement_date</th>\n",
              "      <th>short_interest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6359902</th>\n",
              "      <td>HAS</td>\n",
              "      <td>1973-02-23</td>\n",
              "      <td>1973-01-15</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1673372</th>\n",
              "      <td>BDL</td>\n",
              "      <td>1973-02-23</td>\n",
              "      <td>1973-01-15</td>\n",
              "      <td>600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3918332</th>\n",
              "      <td>DIS</td>\n",
              "      <td>1973-02-23</td>\n",
              "      <td>1973-01-15</td>\n",
              "      <td>142416.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12864148</th>\n",
              "      <td>SPB</td>\n",
              "      <td>1973-03-02</td>\n",
              "      <td>1973-02-15</td>\n",
              "      <td>79663.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1839487</th>\n",
              "      <td>BIO.2</td>\n",
              "      <td>1973-03-02</td>\n",
              "      <td>1973-02-15</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8851674</th>\n",
              "      <td>MGDDY</td>\n",
              "      <td>2024-10-04</td>\n",
              "      <td>2024-09-13</td>\n",
              "      <td>142892.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15253523</th>\n",
              "      <td>XLY</td>\n",
              "      <td>2024-10-04</td>\n",
              "      <td>2024-09-13</td>\n",
              "      <td>4122425.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7756013</th>\n",
              "      <td>JRSS</td>\n",
              "      <td>2024-10-04</td>\n",
              "      <td>2024-09-13</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4976179</th>\n",
              "      <td>FAF</td>\n",
              "      <td>2024-10-04</td>\n",
              "      <td>2024-09-13</td>\n",
              "      <td>1475937.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14409437</th>\n",
              "      <td>VACNY</td>\n",
              "      <td>2024-10-04</td>\n",
              "      <td>2024-09-13</td>\n",
              "      <td>13676.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         ticker       date settlement_date  short_interest\n",
              "6359902     HAS 1973-02-23      1973-01-15           100.0\n",
              "1673372     BDL 1973-02-23      1973-01-15           600.0\n",
              "3918332     DIS 1973-02-23      1973-01-15        142416.0\n",
              "12864148    SPB 1973-03-02      1973-02-15         79663.0\n",
              "1839487   BIO.2 1973-03-02      1973-02-15           100.0\n",
              "...         ...        ...             ...             ...\n",
              "8851674   MGDDY 2024-10-04      2024-09-13        142892.0\n",
              "15253523    XLY 2024-10-04      2024-09-13       4122425.0\n",
              "7756013    JRSS 2024-10-04      2024-09-13             6.0\n",
              "4976179     FAF 2024-10-04      2024-09-13       1475937.0\n",
              "14409437  VACNY 2024-10-04      2024-09-13         13676.0\n",
              "\n",
              "[100000 rows x 4 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_short.sample(100000).sort_values(\"date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "m9OPwX8AUAOB"
      },
      "outputs": [],
      "source": [
        "\n",
        "import polars as pl\n",
        "\n",
        "# Convert pandas DataFrame to polars DataFrame\n",
        "df_pricing_pol = pl.from_pandas(df_pricing)\n",
        "\n",
        "# Sort by ticker and date\n",
        "df_pricing_pol = df_pricing_pol.sort([\"ticker\", \"date\"])\n",
        "\n",
        "# Add daily features\n",
        "df_pricing_pol = df_pricing_pol.with_columns(\n",
        "    ((pl.col(\"high\") - pl.col(\"low\")) / pl.col(\"open\")).alias(\"daily_volatility\"),\n",
        "    ((pl.col(\"close\") - pl.col(\"close\").shift(1)) / pl.col(\"close\").shift(1)).alias(\"return_daily\"),\n",
        "    (pl.col(\"volume\") / pl.col(\"volume\").rolling_mean(window_size=20).over(\"ticker\")).alias(\"relative_volume\"),\n",
        ")\n",
        "\n",
        "### looks good to me, I think the problem here is nothing but ratios!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TQmnG68FUAOB",
        "outputId": "2860767b-afb4-4882-9d7b-97729e4182a7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (44_058_202, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ticker</th><th>date</th><th>open</th><th>high</th><th>low</th><th>close</th><th>volume</th><th>daily_volatility</th><th>return_daily</th><th>relative_volume</th></tr><tr><td>str</td><td>datetime[ns]</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>&quot;A1&quot;</td><td>1997-12-31 00:00:00</td><td>17.188</td><td>17.5</td><td>17.188</td><td>17.188</td><td>71800.0</td><td>0.018152</td><td>-0.881093</td><td>null</td></tr><tr><td>&quot;AAB&quot;</td><td>1997-12-31 00:00:00</td><td>16.375</td><td>16.5</td><td>15.875</td><td>16.5</td><td>7900.0</td><td>0.038168</td><td>-0.797868</td><td>null</td></tr><tr><td>&quot;AABC&quot;</td><td>1997-12-31 00:00:00</td><td>11.0</td><td>11.0</td><td>11.0</td><td>11.0</td><td>2000.0</td><td>0.0</td><td>-0.524324</td><td>null</td></tr><tr><td>&quot;AAC1&quot;</td><td>1997-12-31 00:00:00</td><td>6.625</td><td>7.5</td><td>6.625</td><td>7.438</td><td>1.0664e6</td><td>0.132075</td><td>10.80635</td><td>null</td></tr><tr><td>&quot;AACB&quot;</td><td>1997-12-31 00:00:00</td><td>11.625</td><td>12.75</td><td>11.625</td><td>12.75</td><td>10600.0</td><td>0.096774</td><td>1.582017</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;ZVRA&quot;</td><td>2024-10-03 00:00:00</td><td>7.05</td><td>7.215</td><td>6.9</td><td>7.18</td><td>436757.0</td><td>0.044681</td><td>0.008427</td><td>0.350991</td></tr><tr><td>&quot;ZVSA&quot;</td><td>2024-10-03 00:00:00</td><td>2.31</td><td>2.326</td><td>2.21</td><td>2.26</td><td>28071.0</td><td>0.050216</td><td>-0.008772</td><td>0.386361</td></tr><tr><td>&quot;ZWS&quot;</td><td>2024-10-03 00:00:00</td><td>36.139999</td><td>36.5</td><td>36.005001</td><td>36.130001</td><td>1.461848e6</td><td>0.013697</td><td>-0.00386</td><td>0.521552</td></tr><tr><td>&quot;ZYME&quot;</td><td>2024-10-03 00:00:00</td><td>12.35</td><td>12.47</td><td>11.99</td><td>12.19</td><td>550832.0</td><td>0.038866</td><td>-0.016142</td><td>1.112456</td></tr><tr><td>&quot;ZYXI&quot;</td><td>2024-10-03 00:00:00</td><td>7.95</td><td>7.95</td><td>7.699</td><td>7.83</td><td>55700.0</td><td>0.031572</td><td>-0.015094</td><td>0.590129</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (44_058_202, 10)\n",
              "┌────────┬─────────────┬───────────┬───────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
              "│ ticker ┆ date        ┆ open      ┆ high  ┆ … ┆ volume     ┆ daily_vola ┆ return_dai ┆ relative_v │\n",
              "│ ---    ┆ ---         ┆ ---       ┆ ---   ┆   ┆ ---        ┆ tility     ┆ ly         ┆ olume      │\n",
              "│ str    ┆ datetime[ns ┆ f32       ┆ f32   ┆   ┆ f32        ┆ ---        ┆ ---        ┆ ---        │\n",
              "│        ┆ ]           ┆           ┆       ┆   ┆            ┆ f32        ┆ f32        ┆ f32        │\n",
              "╞════════╪═════════════╪═══════════╪═══════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
              "│ A1     ┆ 1997-12-31  ┆ 17.188    ┆ 17.5  ┆ … ┆ 71800.0    ┆ 0.018152   ┆ -0.881093  ┆ null       │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "│ AAB    ┆ 1997-12-31  ┆ 16.375    ┆ 16.5  ┆ … ┆ 7900.0     ┆ 0.038168   ┆ -0.797868  ┆ null       │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "│ AABC   ┆ 1997-12-31  ┆ 11.0      ┆ 11.0  ┆ … ┆ 2000.0     ┆ 0.0        ┆ -0.524324  ┆ null       │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "│ AAC1   ┆ 1997-12-31  ┆ 6.625     ┆ 7.5   ┆ … ┆ 1.0664e6   ┆ 0.132075   ┆ 10.80635   ┆ null       │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "│ AACB   ┆ 1997-12-31  ┆ 11.625    ┆ 12.75 ┆ … ┆ 10600.0    ┆ 0.096774   ┆ 1.582017   ┆ null       │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "│ …      ┆ …           ┆ …         ┆ …     ┆ … ┆ …          ┆ …          ┆ …          ┆ …          │\n",
              "│ ZVRA   ┆ 2024-10-03  ┆ 7.05      ┆ 7.215 ┆ … ┆ 436757.0   ┆ 0.044681   ┆ 0.008427   ┆ 0.350991   │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "│ ZVSA   ┆ 2024-10-03  ┆ 2.31      ┆ 2.326 ┆ … ┆ 28071.0    ┆ 0.050216   ┆ -0.008772  ┆ 0.386361   │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "│ ZWS    ┆ 2024-10-03  ┆ 36.139999 ┆ 36.5  ┆ … ┆ 1.461848e6 ┆ 0.013697   ┆ -0.00386   ┆ 0.521552   │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "│ ZYME   ┆ 2024-10-03  ┆ 12.35     ┆ 12.47 ┆ … ┆ 550832.0   ┆ 0.038866   ┆ -0.016142  ┆ 1.112456   │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "│ ZYXI   ┆ 2024-10-03  ┆ 7.95      ┆ 7.95  ┆ … ┆ 55700.0    ┆ 0.031572   ┆ -0.015094  ┆ 0.590129   │\n",
              "│        ┆ 00:00:00    ┆           ┆       ┆   ┆            ┆            ┆            ┆            │\n",
              "└────────┴─────────────┴───────────┴───────┴───┴────────────┴────────────┴────────────┴────────────┘"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pricing_pol.sort(\"date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Qf2iS9UOUAOB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Resample to weekly level (end of week - Friday)\n",
        "df_weekly = (\n",
        "    df_pricing_pol\n",
        "    .with_columns(\n",
        "        pl.col(\"date\").dt.truncate(\"1w\").dt.offset_by(\"4d\")\n",
        "    )\n",
        "    .group_by([\"ticker\", \"date\"])\n",
        "    .agg(\n",
        "        pl.first(\"open\").alias(\"weekly_open\"),\n",
        "        pl.max(\"high\").alias(\"weekly_high\"),\n",
        "        pl.min(\"low\").alias(\"weekly_low\"),\n",
        "        pl.last(\"close\").alias(\"weekly_close\"),\n",
        "        ((pl.col(\"volume\") * pl.col(\"close\")).sum() / pl.col(\"volume\").sum()).alias(\"weekly_vwap\"),\n",
        "        pl.sum(\"volume\").alias(\"weekly_volume\"),\n",
        "        pl.mean(\"daily_volatility\").alias(\"avg_daily_volatility\"),\n",
        "        pl.mean(\"return_daily\").alias(\"avg_return_daily\"),\n",
        "        pl.mean(\"relative_volume\").alias(\"avg_relative_volume\"),\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "rSXwXYthUAOB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate weekly return\n",
        "df_weekly = df_weekly.with_columns(\n",
        "    ((pl.col(\"weekly_close\") - pl.col(\"weekly_open\")) / pl.col(\"weekly_open\")).alias(\"weekly_return\")\n",
        ")\n",
        "\n",
        "# Calculate longer-term returns\n",
        "df_weekly = df_weekly.with_columns(\n",
        "    ((pl.col(\"weekly_close\") / pl.col(\"weekly_close\").shift(12)) - 1).over(\"ticker\").alias(\"12week_return\"),\n",
        "    ((pl.col(\"weekly_close\") / pl.col(\"weekly_close\").shift(26)) - 1).over(\"ticker\").alias(\"26week_return\"),\n",
        "    ((pl.col(\"weekly_close\") / pl.col(\"weekly_close\").shift(52)) - 1).over(\"ticker\").alias(\"52week_return\"),\n",
        ")\n",
        "\n",
        "# Calculate changes in volume\n",
        "df_weekly = df_weekly.with_columns(\n",
        "    ((pl.col(\"weekly_volume\") / pl.col(\"weekly_volume\").shift(12)) - 1).over(\"ticker\").alias(\"12week_volume_change\"),\n",
        "    ((pl.col(\"weekly_volume\") / pl.col(\"weekly_volume\").shift(26)) - 1).over(\"ticker\").alias(\"26week_volume_change\"),\n",
        "    ((pl.col(\"weekly_volume\") / pl.col(\"weekly_volume\").shift(52)) - 1).over(\"ticker\").alias(\"52week_volume_change\"),\n",
        ")\n",
        "\n",
        "# Calculate log returns (optional)\n",
        "df_weekly = df_weekly.with_columns(\n",
        "    (pl.col(\"weekly_close\") / pl.col(\"weekly_close\").shift(12)).log().over(\"ticker\").alias(\"12week_log_return\"),\n",
        "    (pl.col(\"weekly_close\") / pl.col(\"weekly_close\").shift(26)).log().over(\"ticker\").alias(\"26week_log_return\"),\n",
        "    (pl.col(\"weekly_close\") / pl.col(\"weekly_close\").shift(52)).log().over(\"ticker\").alias(\"52week_log_return\"),\n",
        ")\n",
        "\n",
        "window_size = 52  # Assuming weekly data, 52 weeks = 1 year\n",
        "risk_free_rate = 0.02  # Assuming a 2% risk-free rate (adjust as needed)\n",
        "\n",
        "df_weekly = df_weekly.with_columns(\n",
        "    ((pl.col(\"weekly_return\") - risk_free_rate / 52) / pl.col(\"avg_daily_volatility\")).rolling_mean(window_size=window_size).over(\"ticker\").alias(f\"rolling_sharpe_ratio_{window_size}\")\n",
        ")\n",
        "\n",
        "# Calculate rolling standard deviations for price and volume\n",
        "window_sizes = [12, 26, 52]  # Adjust the window sizes as needed\n",
        "\n",
        "for window_size in window_sizes:\n",
        "    df_weekly = df_weekly.with_columns(\n",
        "        pl.col(\"weekly_close\").rolling_std(window_size=window_size).over(\"ticker\").alias(f\"rolling_price_std_{window_size}\"),\n",
        "        pl.col(\"weekly_volume\").rolling_std(window_size=window_size).over(\"ticker\").alias(f\"rolling_volume_std_{window_size}\")\n",
        "    )\n",
        "\n",
        "# Calculate rolling drawdowns\n",
        "for window_size in window_sizes:\n",
        "    df_weekly = df_weekly.with_columns(\n",
        "        ((pl.col(\"weekly_close\").rolling_max(window_size).over(\"ticker\") - pl.col(\"weekly_close\")) / pl.col(\"weekly_close\").rolling_max(window_size).over(\"ticker\")).alias(f\"rolling_drawdown_{window_size}\")\n",
        "    )\n",
        "\n",
        "df_weekly = df_weekly.with_columns(\n",
        "    pl.col(\"weekly_volume\").rolling_mean(window_size=8).over(\"ticker\").alias(\"rolling_volume_mean_8\")\n",
        ")\n",
        "\n",
        "df_weekly = df_weekly.sort([\"ticker\", \"date\"])\n",
        "df_weekly = df_weekly.to_pandas()\n",
        "#### GRAB RATIO DATA\n",
        "\n",
        "df_pricing_week = pd.read_parquet(\"gs://sovai-accounting/processed/ratios_weekly_interpolated.parq\")\n",
        "df_pricing_week = df_pricing_week.merge(df_short.set_index([\"ticker\",\"date\"]), left_index=True, right_index=True, how=\"left\")\n",
        "df_pricing_week = df_pricing_week.groupby(\"ticker\").ffill()\n",
        "df_pricing_week = df_pricing_week.reset_index()\n",
        "df_pricing_week = df_pricing_week.dropna(subset=[\"settlement_date\",\"short_interest\"])\n",
        "df_pricing_week = df_pricing_week.set_index([\"ticker\",\"date\"])\n",
        "\n",
        "\n",
        "df_weekly = df_weekly.set_index([\"ticker\",\"date\"])\n",
        "df_pricing_week = df_weekly.merge(df_pricing_week, left_index=True, right_index=True, how=\"left\")\n",
        "del df_weekly\n",
        "df_pricing_week = df_pricing_week.dropna(subset=[\"short_interest\",\"market_cap\"])\n",
        "df_pricing_week[\"number_of_shares\"] = df_pricing_week[\"market_cap\"]/df_pricing_week[\"weekly_close\"]\n",
        "df_pricing_week[\"short_to_float\"] = df_pricing_week[\"short_interest\"]/(df_pricing_week[\"number_of_shares\"]*1000000)\n",
        "# Convert 'short_to_float' column to numeric data type\n",
        "df_pricing_week['short_to_float'] = pd.to_numeric(df_pricing_week['short_to_float'], errors='coerce')\n",
        "\n",
        "df_pricing_week[\"short_to_float\"] = df_pricing_week[\"short_to_float\"].replace(\"NaN\", np.nan)\n",
        "\n",
        "# Drop rows where 'short_to_float' is NaN\n",
        "df_pricing_week = df_pricing_week.dropna(subset=['short_to_float'])\n",
        "\n",
        "df_pricing_week = df_pricing_week.drop(columns=[\"settlement_date\"])\n",
        "\n",
        "#### GRAB ACCOUNTING DATA\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the GCS path to the parquet file\n",
        "gcs_path = \"gs://sovai-accounting/processed/accounting_weekly_interpolated.parq\"\n",
        "\n",
        "top_20_features = [\n",
        "    'total_revenue',\n",
        "    'net_income',\n",
        "    'total_assets',\n",
        "    'total_liabilities',\n",
        "    'total_equity',\n",
        "    'operating_income',\n",
        "    'free_cash_flow',\n",
        "    'ebitda',\n",
        "    'current_assets',\n",
        "    'current_liabilities',\n",
        "    'operating_expenses',\n",
        "    'net_cash_flow',\n",
        "    'cost_of_revenue',\n",
        "    'total_debt',\n",
        "    'gross_profit',\n",
        "    'net_cash_flow_operating',\n",
        "    'net_cash_flow_investing',\n",
        "    'net_cash_flow_financing',\n",
        "    'working_capital',\n",
        "    'enterprise_value'\n",
        "]\n",
        "\n",
        "# Read the specified columns from the parquet file\n",
        "df_accounting = pd.read_parquet(gcs_path, columns=top_20_features)\n",
        "df_pricing_week = df_pricing_week.merge(df_accounting, left_index=True, right_index=True, how=\"left\")\n",
        "#### GRAB META DATA\n",
        "\n",
        "df_ticker = pd.read_parquet(\"gs://sovai-accounting/dataframes/tickers.parq\")\n",
        "\n",
        "def replace_small_cat(df, columns, thresh=0.005, term=\"Other\"):\n",
        "    for col in columns:\n",
        "        frequencies = df[col].value_counts(normalize=True)\n",
        "        small_categories = frequencies[frequencies < thresh].index\n",
        "        df[col] = df[col].replace(small_categories, \"Other\")\n",
        "    return df\n",
        "\n",
        "features = [\"sicindustry\", \"sector\", \"industry\", \"exchange\", \"currency\", \"location\"]\n",
        "df_ticker = replace_small_cat(df_ticker, features)\n",
        "df_ticker = df_ticker[[\"ticker\",\"sicindustry\", \"sector\", \"industry\", \"exchange\", \"currency\", \"location\"]]\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df_ticker[features] = df_ticker[features].apply(LabelEncoder().fit_transform)\n",
        "\n",
        "df_ticker = df_ticker.drop_duplicates([\"ticker\"],keep=\"first\")\n",
        "df_ticker = df_ticker[~df_ticker[\"ticker\"].isnull()]\n",
        "\n",
        "df_pricing_week = pd.merge(df_pricing_week.reset_index(), df_ticker, on=\"ticker\", how=\"left\")\n",
        "df_pricing_week['year'] = df_pricing_week['date'].dt.year\n",
        "df_pricing_week['quarter'] = df_pricing_week['date'].dt.quarter\n",
        "df_pricing_week = df_pricing_week.set_index([\"ticker\",\"date\"])\n",
        "\n",
        "df_pricing_week[\"short_to_float\"] = df_pricing_week[\"short_to_float\"].groupby('date').transform(lambda x: x.rank(pct=True))\n",
        "\n",
        "# df_pricing_week = df_pricing_week.query(\"ticker == 'AAPL'\")\n",
        "df_pricing_week.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AA7nxN31NRF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "# Convert the DataFrame to a Polars DataFrame\n",
        "df_pricing_week = pl.from_pandas(df_pricing_week.reset_index(drop=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuYzWQwLAx50",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "# Calculate the percentage change over 52 weeks for the selected features\n",
        "pct_change_features = [\n",
        "    'short_to_float', 'total_revenue', 'net_income', 'total_assets', 'total_liabilities',\n",
        "    'total_equity', 'operating_income', 'free_cash_flow', 'ebitda', 'current_assets',\n",
        "    'current_liabilities', 'operating_expenses', 'net_cash_flow', 'cost_of_revenue', 'total_debt',\n",
        "    'current_ratio',\n",
        "    'debt_equity_ratio',\n",
        "    'return_on_assets',\n",
        "    'gross_profit_margin',\n",
        "    'price_to_earnings',\n",
        "    'asset_turnover',\n",
        "    'operating_cash_flow_to_sales',\n",
        "    'debt_to_capital',\n",
        "    'ebitda_margin',\n",
        "]\n",
        "\n",
        "for feature in pct_change_features:\n",
        "    df_pricing_week = df_pricing_week.with_columns([\n",
        "        pl.col(feature).shift(52).over('ticker').pct_change().alias(f\"{feature}_pct_change_52w\")\n",
        "    ])\n",
        "\n",
        "# Calculate the standard deviation over 52 weeks for the selected features\n",
        "std_features = [\n",
        "    'short_to_float', '12week_return',  '12week_volume_change','rolling_drawdown_12'\n",
        "]\n",
        "\n",
        "for feature in std_features:\n",
        "    df_pricing_week = df_pricing_week.with_columns([\n",
        "        pl.col(feature).rolling_std(window_size=52).over('ticker').alias(f\"{feature}_std_52w\")\n",
        "    ])\n",
        "\n",
        "df_pricing_week = df_pricing_week.sort([\"ticker\", \"date\"])\n",
        "\n",
        "# Convert the Polars DataFrame back to a Pandas DataFrame\n",
        "df_pricing_week = df_pricing_week.to_pandas()\n",
        "df_pricing_week = df_pricing_week.set_index([\"ticker\",\"date\"])\n",
        "df_pricing_week = df_pricing_week.dropna(subset=[\"short_to_float\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WepWaBH1UAOB"
      },
      "outputs": [],
      "source": [
        "df_pricing_week.sample(100000).sort_values(\"date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5z_mEqzwbXZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming df_pricing_week is already preprocessed and contains only numeric columns\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = df_pricing_week.drop(['short_to_float','short_interest'], axis=1)\n",
        "y = df_pricing_week['short_to_float']\n",
        "\n",
        "# Create LightGBM dataset\n",
        "train_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Specify the parameters for the LightGBM model\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'mse',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'verbose': 1\n",
        "}\n",
        "\n",
        "# Train the LightGBM model\n",
        "model = lgb.train(params, train_data)\n",
        "\n",
        "# Make predictions on the entire dataset\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse = mean_squared_error(y, predictions)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# Add the predicted values as a new column in the DataFrame\n",
        "df_pricing_week['predicted_short_to_float'] = predictions\n",
        "\n",
        "df_pricing_week['overshorted'] = df_pricing_week['short_to_float'] - df_pricing_week['predicted_short_to_float']\n",
        "\n",
        "## Some feature explanations too\n",
        "\n",
        "# Take a sample of 10,000 rows from X\n",
        "sample_X = X.sample(n=10000, random_state=42)\n",
        "\n",
        "# Calculate SHAP values for the sample\n",
        "lgbm_shap = model.predict(sample_X, pred_contrib=True)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = sample_X.columns.tolist()\n",
        "\n",
        "# Create a DataFrame with feature names and SHAP values\n",
        "shap_df = pd.DataFrame(lgbm_shap[:, :-1], columns=feature_names)\n",
        "\n",
        "# Calculate the mean absolute SHAP value for each feature\n",
        "shap_importance = shap_df.abs().mean()\n",
        "\n",
        "# Sort the features by their mean absolute SHAP value in descending order\n",
        "shap_importance = shap_importance.sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5UYBBR1TUAOB"
      },
      "outputs": [],
      "source": [
        "shap_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awp8LVbvWXfz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_pricing_week.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9OCOWNXcUAOC"
      },
      "outputs": [],
      "source": [
        "df_pricing_week[\"rolling_volume_mean_8\"] = df_pricing_week[\"rolling_volume_mean_8\"].fillna(df_pricing_week[\"volume\"])\n",
        "\n",
        "df_pricing_week[\"days_to_cover\"] = df_pricing_week[\"rolling_volume_mean_8\"]/df_pricing_week[\"short_interest\"]\n",
        "\n",
        "# df_excerpt = df_excerpt.drop(columns=[\"rolling_volume_mean_8\"])\n",
        "\n",
        "df_pricing_week[\"number_of_shares\"] = df_pricing_week[\"number_of_shares\"]*1000000\n",
        "\n",
        "df_pricing_week = df_pricing_week.rename(columns={\"short_to_float\":\"short_percentage\",\"predicted_short_to_float\":\"short_prediction\",\"overshorted\":\"over_shorted\"})\n",
        "\n",
        "# Assuming your DataFrame is named 'df_excerpt'\n",
        "df_pricing_week['over_shorted_chg'] = df_pricing_week[['over_shorted']].groupby('ticker')['over_shorted'].diff()\n",
        "\n",
        "df_pricing_week = df_pricing_week.astype(\"float32\")\n",
        "\n",
        "df_pricing_week = df_pricing_week.round(3)\n",
        "\n",
        "df_pricing_week.columns = df_pricing_week.columns.str.lower()\n",
        "\n",
        "data_list = [\n",
        "    \"over_shorted\",\n",
        "    \"over_shorted_chg\",\n",
        "    \"short_interest\",\n",
        "    \"number_of_shares\",\n",
        "    \"short_percentage\",\n",
        "    \"short_prediction\",\n",
        "    \"days_to_cover\",\n",
        "    \"market_cap\",\n",
        "    \"total_revenue\",\n",
        "    \"volume\",\n",
        "]\n",
        "\n",
        "df_excerpt = df_pricing_week[data_list].copy()\n",
        "df_excerpt = df_excerpt.fillna(0)\n",
        "\n",
        "df_excerpt.to_parquet(\"gs://sovai-short/processed/over_shorted_frame.parquet\")\n",
        "\n",
        "df_excerpt = df_excerpt.reset_index(drop=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pricing_week.to_parquet(\"gs://sovai-short/processed/feature_store_short.parquet\")"
      ],
      "metadata": {
        "id": "XiNHgQUMWGGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import duckdb\n",
        "import polars as pl\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# Configure logger\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "DO_BUCKET = \"sovai\"\n",
        "DO_ACCESS_KEY = \"DO00JLGWA2N8GYY7CTKZ\"\n",
        "DO_SECRET_KEY = \"YvDuf4HWXmrj792i5upL7EkB0I3kplMxEEyl4fpz4EE\"\n",
        "ENDPOINT = \"nyc3.digitaloceanspaces.com\"\n",
        "\n",
        "def save_local_partition_duckdb(df: pl.DataFrame, partition_cols: list, local_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Save a Polars DataFrame as a partitioned Parquet dataset locally using DuckDB.\n",
        "    The data is sorted by the partition column(s) to group partition data together.\n",
        "    \"\"\"\n",
        "    os.makedirs(local_dir, exist_ok=True)\n",
        "    con = duckdb.connect(database=':memory:')\n",
        "\n",
        "    partition_by = \",\".join(partition_cols)\n",
        "    sql = f\"\"\"\n",
        "    COPY (\n",
        "        SELECT * FROM df\n",
        "        ORDER BY {partition_by}\n",
        "    )\n",
        "    TO '{local_dir}'\n",
        "    (OVERWRITE, FORMAT 'parquet', PARTITION_BY '{partition_by}')\n",
        "    \"\"\"\n",
        "    con.execute(sql)\n",
        "    message = f\"Local partitioning complete. Files written to: {local_dir}\"\n",
        "    logger.info(message)\n",
        "    print(message)\n",
        "\n",
        "def s5cmd_sync_directory(local_dir: str, do_bucket: str, remote_subdir: str,\n",
        "                         do_access_key: str, do_secret_key: str, endpoint: str,\n",
        "                         numworkers: int = 256, concurrency: int = 5) -> None:\n",
        "    \"\"\"\n",
        "    Sync the local directory to DigitalOcean Spaces using s5cmd's sync command.\n",
        "    Ensures that the local source directory ends with a trailing slash so that s5cmd\n",
        "    syncs its contents rather than the full directory path.\n",
        "    \"\"\"\n",
        "    if not local_dir.endswith('/'):\n",
        "        local_dir += '/'\n",
        "\n",
        "    cmd = [\n",
        "        \"s5cmd\",\n",
        "        \"--endpoint-url\", f\"https://{endpoint}\",\n",
        "        \"--numworkers\", str(numworkers),\n",
        "        \"sync\",\n",
        "        \"--size-only\",\n",
        "        \"--concurrency\", str(concurrency),\n",
        "        local_dir,\n",
        "        f\"s3://{do_bucket}/{remote_subdir}/\",\n",
        "    ]\n",
        "    message = f\"Syncing {local_dir} to s3://{do_bucket}/{remote_subdir}/ using s5cmd sync \" \\\n",
        "              f\"with {numworkers} workers and {concurrency} concurrency.\"\n",
        "    logger.info(message)\n",
        "    print(message)\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env[\"AWS_ACCESS_KEY_ID\"] = do_access_key\n",
        "    env[\"AWS_SECRET_ACCESS_KEY\"] = do_secret_key\n",
        "    env[\"AWS_REGION\"] = endpoint.split('.')[0]  # e.g., 'nyc3'\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        if result.returncode != 0:\n",
        "            error_message = f\"s5cmd sync failed:\\n{result.stderr}\"\n",
        "            logger.error(error_message)\n",
        "            print(error_message)\n",
        "            raise subprocess.CalledProcessError(result.returncode, cmd, output=result.stdout, stderr=result.stderr)\n",
        "        else:\n",
        "            message = \"s5cmd sync completed successfully.\"\n",
        "            logger.info(message)\n",
        "            print(message)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error_message = f\"s5cmd sync raised an error: {e}\"\n",
        "        logger.error(error_message)\n",
        "        print(error_message)\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "datasets_config_normal = {\n",
        "    \"over_shorted\": {\n",
        "        \"partitions\": [\"ticker\", \"date\"],  # Added date partition\n",
        "        \"local_dirs\": {\n",
        "            \"ticker\": \"/tmp/over_shorted/partitioned_data/ticker/\",\n",
        "            \"date\": \"/tmp/over_shorted/partitioned_data/date/\"  # Added date directory\n",
        "        },\n",
        "        \"remote_dirs\": {\n",
        "            \"ticker\": \"sovai-short/over_shorted/partitioned/ticker\",\n",
        "            \"date\": \"sovai-short/over_shorted/partitioned/date\"  # Added date directory\n",
        "        }\n",
        "    },\n",
        "}\n",
        "\n",
        "df = pl.read_parquet(\"gs://sovai-short/processed/over_shorted_frame.parquet\")\n",
        "\n",
        "# df = pl.from_pandas(simple_maker)\n",
        "df = df.filter(pl.col(\"ticker\").is_not_null()).select(\n",
        "    \"ticker\", \"date\", pl.all().exclude([\"ticker\", \"date\"])\n",
        ")\n",
        "\n",
        "# Process each dataset sequentially.\n",
        "for dataset_name, cfg in datasets_config_normal.items():\n",
        "    logger.info(f\"Processing dataset: {dataset_name}\")\n",
        "    print(f\"Processing dataset: {dataset_name}\")\n",
        "\n",
        "    # Read dataset from the specified file.\n",
        "    # df = pl.read_parquet(cfg[\"file\"])\n",
        "\n",
        "\n",
        "    # If the dataset is partitioned by ticker, filter out null values.\n",
        "    if \"ticker\" in cfg[\"partitions\"]:\n",
        "        df = df.filter(pl.col(\"ticker\").is_not_null())\n",
        "\n",
        "    # Add partition columns. For 'ticker', just alias; for 'date' apply a transformation.\n",
        "    if \"ticker\" in cfg[\"partitions\"]:\n",
        "        df = df.with_columns(pl.col(\"ticker\").alias(\"ticker_partitioned\"))\n",
        "    if \"date\" in cfg[\"partitions\"]:\n",
        "        df = df.with_columns(\n",
        "            pl.col(\"date\")\n",
        "              .dt.to_string(\"%G-%V-5\")   # Format ISO year and week with a fixed weekday (e.g. Friday)\n",
        "              .str.to_date(\"%G-%V-%u\")    # Convert back to a Date using ISO weekday\n",
        "              .alias(\"date_partitioned\")\n",
        "        )\n",
        "\n",
        "    # Process each partition type for this dataset.\n",
        "    for part in cfg[\"partitions\"]:\n",
        "        partition_col = f\"{part}_partitioned\"\n",
        "        # If there are multiple partition columns, drop the ones not used for this run.\n",
        "        current_df = df\n",
        "        if len(cfg[\"partitions\"]) > 1:\n",
        "            drop_cols = [f\"{p}_partitioned\" for p in cfg[\"partitions\"] if p != part]\n",
        "            current_df = current_df.drop(drop_cols)\n",
        "\n",
        "        local_dir = cfg[\"local_dirs\"][part]\n",
        "        remote_dir = cfg[\"remote_dirs\"][part]\n",
        "\n",
        "        logger.info(f\"Partitioning {dataset_name} by '{part}'. Local dir: {local_dir}, Remote dir: {remote_dir}\")\n",
        "        print(f\"Partitioning {dataset_name} by '{part}'. Local dir: {local_dir}, Remote dir: {remote_dir}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        save_local_partition_duckdb(current_df, [partition_col], local_dir)\n",
        "        elapsed = time.time() - start_time\n",
        "        message = f\"{dataset_name} partitioning by '{part}' took {elapsed:.2f} seconds.\"\n",
        "        logger.info(message)\n",
        "        print(message)\n",
        "\n",
        "        start_time = time.time()\n",
        "        s5cmd_sync_directory(local_dir, DO_BUCKET, remote_dir,\n",
        "                            DO_ACCESS_KEY, DO_SECRET_KEY, ENDPOINT,\n",
        "                            numworkers=24, concurrency=5)\n",
        "        elapsed = time.time() - start_time\n",
        "        message = f\"s5cmd sync for {dataset_name} partitioning by '{part}' took {elapsed:.2f} seconds.\"\n",
        "        logger.info(message)\n",
        "        print(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320,
          "referenced_widgets": [
            "f6da436078494fca8db13cd45b571bf8",
            "0c0bf42036314c07ba8e678e8e6dbc08",
            "843db70b7ebe484493ffe4fb6d60489b",
            "d18e073b34a247688eb6787b5c09c87f",
            "6cd52faff5ed4940868be1f21eb581f5",
            "34fcbf9e727742d7a1190cc19a7d94d8"
          ]
        },
        "id": "cfacAgz-WRKs",
        "outputId": "b8d28a51-0d83-4331-c002-60554aec318f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: over_shorted\n",
            "Partitioning over_shorted by 'ticker'. Local dir: /tmp/over_shorted/partitioned_data/ticker/, Remote dir: sovai-short/over_shorted/partitioned/ticker\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6da436078494fca8db13cd45b571bf8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local partitioning complete. Files written to: /tmp/over_shorted/partitioned_data/ticker/\n",
            "over_shorted partitioning by 'ticker' took 6.53 seconds.\n",
            "Syncing /tmp/over_shorted/partitioned_data/ticker/ to s3://sovai/sovai-short/over_shorted/partitioned/ticker/ using s5cmd sync with 24 workers and 5 concurrency.\n",
            "s5cmd sync completed successfully.\n",
            "s5cmd sync for over_shorted partitioning by 'ticker' took 67.11 seconds.\n",
            "Partitioning over_shorted by 'date'. Local dir: /tmp/over_shorted/partitioned_data/date/, Remote dir: sovai-short/over_shorted/partitioned/date\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d18e073b34a247688eb6787b5c09c87f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local partitioning complete. Files written to: /tmp/over_shorted/partitioned_data/date/\n",
            "over_shorted partitioning by 'date' took 2.22 seconds.\n",
            "Syncing /tmp/over_shorted/partitioned_data/date/ to s3://sovai/sovai-short/over_shorted/partitioned/date/ using s5cmd sync with 24 workers and 5 concurrency.\n",
            "s5cmd sync completed successfully.\n",
            "s5cmd sync for over_shorted partitioning by 'date' took 9.10 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Ojv8HQuNUAOC"
      },
      "outputs": [],
      "source": [
        "# ##\n",
        "# from google.cloud import storage\n",
        "# import pandas as pd\n",
        "# import os\n",
        "# import tempfile\n",
        "# from io import StringIO\n",
        "# import numpy as np\n",
        "# import psycopg2\n",
        "# from psycopg2 import sql\n",
        "# import time\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# # Credentials\n",
        "# username = 'postgres'\n",
        "# password = 'Phithae1eeja1oap'\n",
        "# host = '35.192.1.147'\n",
        "# port = '5432'\n",
        "# database = 'altdata'\n",
        "# schema = 'short'\n",
        "# chunksize = 100000\n",
        "\n",
        "# # Define TCP keepalive parameters\n",
        "# keepalive_kwargs = {\n",
        "#   \"keepalives\": 1,\n",
        "#   \"keepalives_idle\": 1200,\n",
        "#   \"keepalives_interval\": 10,\n",
        "#   \"keepalives_count\": 5\n",
        "# }\n",
        "\n",
        "# # Define max retries\n",
        "# MAX_RETRIES = 5\n",
        "\n",
        "# def execute_query(cur, conn, query, data=None, retries=MAX_RETRIES):\n",
        "#     print(\"Executing SQL:\", query)  # <-- Print the SQL being executed\n",
        "#     while retries > 0:\n",
        "#         try:\n",
        "#             cur.execute(query, data)\n",
        "#             conn.commit()\n",
        "#             return\n",
        "#         except psycopg2.OperationalError as e:\n",
        "#             if 'SSL SYSCALL' in str(e):\n",
        "#                 print('Connection lost. Retrying...')\n",
        "#                 retries -= 1\n",
        "#                 time.sleep(5)\n",
        "#                 continue\n",
        "#             else:\n",
        "#                 raise e\n",
        "#     raise RuntimeError('Failed to execute query after multiple attempts')\n",
        "\n",
        "# def columns_in_table(cur, table_name):\n",
        "#     cur.execute(f\"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}';\")\n",
        "#     columns = [row[0] for row in cur.fetchall()]\n",
        "#     return columns\n",
        "\n",
        "# dtype_mapping = {\n",
        "#     np.dtype('int32'): 'INTEGER',\n",
        "#     np.dtype('int64'): 'BIGINT',\n",
        "#     np.dtype('float32'): 'REAL',\n",
        "#     np.dtype('float64'): 'DOUBLE PRECISION',\n",
        "#     np.dtype('datetime64[ns]'): 'TIMESTAMP',\n",
        "#     np.dtype('datetime64[D]'): 'DATE',\n",
        "#     np.dtype('datetime64[ns]'): 'TIMESTAMP',\n",
        "#     np.dtype('datetime64[ms]'): 'TIMESTAMP',\n",
        "#     np.dtype('datetime64[s]'): 'TIMESTAMP',\n",
        "#     np.dtype('datetime64[M]'): 'DATE',\n",
        "#     np.dtype('datetime64[Y]'): 'DATE',\n",
        "#     np.dtype('O'): 'TEXT',\n",
        "#     'bool': 'BOOLEAN',\n",
        "#     'object': 'TEXT',\n",
        "#     'int32': 'INTEGER',\n",
        "#     'int64': 'BIGINT',\n",
        "#     'float32': 'REAL',\n",
        "#     'float64': 'DOUBLE PRECISION',\n",
        "#     'datetime64[ns]': 'TIMESTAMP',\n",
        "#     pd.CategoricalDtype(): 'TEXT',\n",
        "# }\n",
        "\n",
        "# def save_df_to_db(df, cur, table_name):\n",
        "#     temp_table_name = f\"{table_name}_temp\"\n",
        "#     cols_with_type = \", \".join(f\"{col} {dtype_mapping[df[col].dtype]}\" for col in df.columns)\n",
        "\n",
        "#     with tempfile.NamedTemporaryFile(delete=True) as temp_file:\n",
        "#         df.to_csv(temp_file.name, sep='\\t', index=False, header=False)\n",
        "#         temp_file.seek(0)\n",
        "#         cur.copy_from(temp_file, f\"{temp_table_name}\", null=\"\", sep='\\t', columns=df.columns)\n",
        "\n",
        "# def send_off_data_to_psql(df_test_summary, conn, name, replace_data=False):\n",
        "#     cur = conn.cursor()\n",
        "\n",
        "#     # Setup schema and table\n",
        "#     cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n",
        "#     cols_with_type = \", \".join(f\"{col} {dtype_mapping[df_test_summary[col].dtype]}\" for col in df_test_summary.columns)\n",
        "\n",
        "#     if replace_data:\n",
        "#         cur.execute(f\"DROP TABLE IF EXISTS {schema}.{name} CASCADE;\")\n",
        "#         cur.execute(f\"CREATE TABLE {name}_temp ({cols_with_type});\")\n",
        "#     else:\n",
        "#         cur.execute(f\"CREATE TABLE IF NOT EXISTS {name}_temp ({cols_with_type});\")\n",
        "\n",
        "#     # Copy chunks\n",
        "#     chunks = [df_test_summary.iloc[i:i+chunksize] for i in range(0, len(df_test_summary), chunksize)]\n",
        "#     for chunk in chunks:\n",
        "#         save_df_to_db(chunk, cur, name)\n",
        "\n",
        "#     # Move and rename table\n",
        "#     if replace_data:\n",
        "#         cur.execute(f\"DROP TABLE IF EXISTS {schema}.{name}_temp CASCADE;\")\n",
        "#         cur.execute(f\"ALTER TABLE {name}_temp SET SCHEMA {schema};\")\n",
        "#         cur.execute(f\"ALTER TABLE {schema}.{name}_temp RENAME TO {name};\")\n",
        "#     else:\n",
        "#         cur.execute(f\"INSERT INTO {schema}.{name} SELECT * FROM {name}_temp ON CONFLICT DO NOTHING;\")\n",
        "#         cur.execute(f\"DROP TABLE IF EXISTS {name}_temp;\")\n",
        "\n",
        "#     conn.commit()\n",
        "\n",
        "#     cur.close()\n",
        "\n",
        "# # connect to your database\n",
        "# conn = psycopg2.connect(\n",
        "#     dbname=database,\n",
        "#     user=username,\n",
        "#     password=password,\n",
        "#     host=host,\n",
        "#     port=port,\n",
        "#     **keepalive_kwargs\n",
        "# )\n",
        "\n",
        "\n",
        "# replace_data = True  # Set to True to replace data, False to append data\n",
        "\n",
        "# if not df_excerpt.empty:\n",
        "#     send_off_data_to_psql(df_excerpt, conn, 'over_shorted', replace_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0FaWZShGUAOC"
      },
      "outputs": [],
      "source": [
        "# df_excerpt.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpDx0Fgu_x9b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# df_excerpt.query(\"ticker=='MSFT'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls3DCv4KUAOC"
      },
      "outputs": [],
      "source": [
        "# df_pricing_week.to_parquet(\"gs://sovai-short/processed/feature_store_short.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de77rC2QATTm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # \"Volume\", \"Short Interest %\", \"Predicted SI %\", \"Overshorted\", \"Change in Overshort\", \"Days to Cover\",\"Market Cap\", \"Total Revenue\"\n",
        "\n",
        "# df_excerpt[[\"short_prediction\",\"short_percentage\"]].query(\"ticker =='META'\").plot()\n",
        "\n",
        "# df_pricing_week[(df_pricing_week['market_cap'] > 1000) & (df_pricing_week['market_cap'] < 10000)].query(\"date=='2024-06-07'\").sort_values(\"overshorted\").head(20)\n",
        "\n",
        "# df_pricing_week[df_pricing_week['market_cap']>50000].query(\"date=='2024-06-07'\").sort_values(\"overshorted\").head(20)\n",
        "\n",
        "# df_pricing_week.query(\"date=='2024-06-07'\").sort_values(\"overshorted\")\n",
        "\n",
        "# df_pricing_week.query(\"ticker=='VRAX'\")[\"overshorted\"].tail(100).plot()\n",
        "\n",
        "# df_pricing_week.query(\"ticker=='TSLA'\")[\"overshorted\"].plot()\n",
        "\n",
        "# # Take a sample of 10,000 rows from X\n",
        "# sample_X = X.sample(n=10000, random_state=42)\n",
        "\n",
        "# # Calculate SHAP values for the sample\n",
        "# lgbm_shap = model.predict(sample_X, pred_contrib=True)\n",
        "\n",
        "# # Get feature names\n",
        "# feature_names = sample_X.columns.tolist()\n",
        "\n",
        "# # Create a DataFrame with feature names and SHAP values\n",
        "# shap_df = pd.DataFrame(lgbm_shap[:, :-1], columns=feature_names)\n",
        "\n",
        "# # Calculate the mean absolute SHAP value for each feature\n",
        "# shap_importance = shap_df.abs().mean()\n",
        "\n",
        "# # Sort the features by their mean absolute SHAP value in descending order\n",
        "# shap_importance = shap_importance.sort_values(ascending=False)\n",
        "\n",
        "\n",
        "# shap_importance\n",
        "\n",
        "# df_pricing_week.query(\"date=='2024-06-07'\").sort_values(\"overshorted\").to_csv(\"check.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# df_pricing_week.head()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m122",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m122"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6da436078494fca8db13cd45b571bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c0bf42036314c07ba8e678e8e6dbc08",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_843db70b7ebe484493ffe4fb6d60489b",
            "value": 100
          }
        },
        "0c0bf42036314c07ba8e678e8e6dbc08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "843db70b7ebe484493ffe4fb6d60489b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "black",
            "description_width": ""
          }
        },
        "d18e073b34a247688eb6787b5c09c87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cd52faff5ed4940868be1f21eb581f5",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34fcbf9e727742d7a1190cc19a7d94d8",
            "value": 100
          }
        },
        "6cd52faff5ed4940868be1f21eb581f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "34fcbf9e727742d7a1190cc19a7d94d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "black",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}